{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from pygam import LinearGAM, s, te\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd_chf    0\n",
      "eur_usd    0\n",
      "dtype: int64\n",
      "['sp500 open', 'sp500 high', 'sp500 low', 'sp500 close', 'sp500 volume', 'sp500 high-low', 'nasdaq open', 'nasdaq high', 'nasdaq low', 'nasdaq close', 'nasdaq volume', 'nasdaq high-low', 'usd_chf', 'eur_usd', 'silver open', 'silver high', 'silver low', 'silver close', 'silver volume', 'silver high-low', 'oil open', 'oil high', 'oil low', 'oil close', 'oil volume', 'oil high-low', 'platinum open', 'platinum high', 'platinum low', 'platinum close', 'platinum volume', 'platinum high-low', 'palladium open', 'palladium high', 'palladium low', 'palladium close', 'palladium volume', 'palladium high-low', 'gold open', 'gold high', 'gold low', 'gold close', 'gold volume', 'year', 'month', 'day', 'day_of_week', 'day_of_year', 'month_sin', 'month_cos']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sp500 open</th>\n",
       "      <th>sp500 high</th>\n",
       "      <th>sp500 low</th>\n",
       "      <th>sp500 close</th>\n",
       "      <th>sp500 volume</th>\n",
       "      <th>sp500 high-low</th>\n",
       "      <th>nasdaq open</th>\n",
       "      <th>nasdaq high</th>\n",
       "      <th>nasdaq low</th>\n",
       "      <th>nasdaq close</th>\n",
       "      <th>...</th>\n",
       "      <th>gold low</th>\n",
       "      <th>gold close</th>\n",
       "      <th>gold volume</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114.49</td>\n",
       "      <td>115.14</td>\n",
       "      <td>114.420</td>\n",
       "      <td>114.93</td>\n",
       "      <td>115646960.0</td>\n",
       "      <td>0.720</td>\n",
       "      <td>46.26</td>\n",
       "      <td>46.520</td>\n",
       "      <td>46.2200</td>\n",
       "      <td>46.39</td>\n",
       "      <td>...</td>\n",
       "      <td>110.79</td>\n",
       "      <td>112.03</td>\n",
       "      <td>18305238.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114.73</td>\n",
       "      <td>114.84</td>\n",
       "      <td>113.200</td>\n",
       "      <td>113.64</td>\n",
       "      <td>212252769.0</td>\n",
       "      <td>1.640</td>\n",
       "      <td>46.46</td>\n",
       "      <td>46.550</td>\n",
       "      <td>45.6500</td>\n",
       "      <td>45.85</td>\n",
       "      <td>...</td>\n",
       "      <td>110.38</td>\n",
       "      <td>110.86</td>\n",
       "      <td>18000724.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>113.62</td>\n",
       "      <td>115.13</td>\n",
       "      <td>113.590</td>\n",
       "      <td>115.06</td>\n",
       "      <td>138671890.0</td>\n",
       "      <td>1.540</td>\n",
       "      <td>45.96</td>\n",
       "      <td>46.640</td>\n",
       "      <td>45.9500</td>\n",
       "      <td>46.59</td>\n",
       "      <td>...</td>\n",
       "      <td>110.83</td>\n",
       "      <td>111.52</td>\n",
       "      <td>10467927.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114.28</td>\n",
       "      <td>114.45</td>\n",
       "      <td>112.980</td>\n",
       "      <td>113.89</td>\n",
       "      <td>216330645.0</td>\n",
       "      <td>1.470</td>\n",
       "      <td>46.27</td>\n",
       "      <td>46.604</td>\n",
       "      <td>45.4300</td>\n",
       "      <td>45.92</td>\n",
       "      <td>...</td>\n",
       "      <td>108.46</td>\n",
       "      <td>108.94</td>\n",
       "      <td>17534231.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>113.92</td>\n",
       "      <td>114.27</td>\n",
       "      <td>111.560</td>\n",
       "      <td>111.70</td>\n",
       "      <td>344747028.0</td>\n",
       "      <td>2.710</td>\n",
       "      <td>46.06</td>\n",
       "      <td>46.350</td>\n",
       "      <td>45.3000</td>\n",
       "      <td>45.49</td>\n",
       "      <td>...</td>\n",
       "      <td>106.61</td>\n",
       "      <td>107.37</td>\n",
       "      <td>25747831.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3710</th>\n",
       "      <td>576.05</td>\n",
       "      <td>580.33</td>\n",
       "      <td>575.910</td>\n",
       "      <td>579.58</td>\n",
       "      <td>42267994.0</td>\n",
       "      <td>4.420</td>\n",
       "      <td>490.74</td>\n",
       "      <td>494.390</td>\n",
       "      <td>490.1700</td>\n",
       "      <td>493.36</td>\n",
       "      <td>...</td>\n",
       "      <td>244.47</td>\n",
       "      <td>245.47</td>\n",
       "      <td>5789546.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>285</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3712</th>\n",
       "      <td>584.59</td>\n",
       "      <td>584.90</td>\n",
       "      <td>578.545</td>\n",
       "      <td>579.78</td>\n",
       "      <td>54203636.0</td>\n",
       "      <td>6.355</td>\n",
       "      <td>497.83</td>\n",
       "      <td>498.500</td>\n",
       "      <td>488.6800</td>\n",
       "      <td>490.85</td>\n",
       "      <td>...</td>\n",
       "      <td>244.53</td>\n",
       "      <td>245.92</td>\n",
       "      <td>5640831.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>289</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3713</th>\n",
       "      <td>579.78</td>\n",
       "      <td>582.83</td>\n",
       "      <td>578.960</td>\n",
       "      <td>582.30</td>\n",
       "      <td>30725436.0</td>\n",
       "      <td>3.870</td>\n",
       "      <td>491.18</td>\n",
       "      <td>491.690</td>\n",
       "      <td>487.5700</td>\n",
       "      <td>490.91</td>\n",
       "      <td>...</td>\n",
       "      <td>246.36</td>\n",
       "      <td>247.15</td>\n",
       "      <td>5431939.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>290</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3714</th>\n",
       "      <td>585.91</td>\n",
       "      <td>586.12</td>\n",
       "      <td>582.160</td>\n",
       "      <td>582.35</td>\n",
       "      <td>34393714.0</td>\n",
       "      <td>3.960</td>\n",
       "      <td>496.44</td>\n",
       "      <td>496.490</td>\n",
       "      <td>491.1901</td>\n",
       "      <td>491.25</td>\n",
       "      <td>...</td>\n",
       "      <td>247.62</td>\n",
       "      <td>248.63</td>\n",
       "      <td>5176170.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>291</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3715</th>\n",
       "      <td>584.07</td>\n",
       "      <td>585.39</td>\n",
       "      <td>582.580</td>\n",
       "      <td>584.59</td>\n",
       "      <td>37416801.0</td>\n",
       "      <td>2.810</td>\n",
       "      <td>494.06</td>\n",
       "      <td>495.570</td>\n",
       "      <td>493.3000</td>\n",
       "      <td>494.47</td>\n",
       "      <td>...</td>\n",
       "      <td>249.90</td>\n",
       "      <td>251.27</td>\n",
       "      <td>7833614.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>292</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3677 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sp500 open  sp500 high  sp500 low  sp500 close  sp500 volume  \\\n",
       "0         114.49      115.14    114.420       114.93   115646960.0   \n",
       "1         114.73      114.84    113.200       113.64   212252769.0   \n",
       "2         113.62      115.13    113.590       115.06   138671890.0   \n",
       "3         114.28      114.45    112.980       113.89   216330645.0   \n",
       "4         113.92      114.27    111.560       111.70   344747028.0   \n",
       "...          ...         ...        ...          ...           ...   \n",
       "3710      576.05      580.33    575.910       579.58    42267994.0   \n",
       "3712      584.59      584.90    578.545       579.78    54203636.0   \n",
       "3713      579.78      582.83    578.960       582.30    30725436.0   \n",
       "3714      585.91      586.12    582.160       582.35    34393714.0   \n",
       "3715      584.07      585.39    582.580       584.59    37416801.0   \n",
       "\n",
       "      sp500 high-low  nasdaq open  nasdaq high  nasdaq low  nasdaq close  ...  \\\n",
       "0              0.720        46.26       46.520     46.2200         46.39  ...   \n",
       "1              1.640        46.46       46.550     45.6500         45.85  ...   \n",
       "2              1.540        45.96       46.640     45.9500         46.59  ...   \n",
       "3              1.470        46.27       46.604     45.4300         45.92  ...   \n",
       "4              2.710        46.06       46.350     45.3000         45.49  ...   \n",
       "...              ...          ...          ...         ...           ...  ...   \n",
       "3710           4.420       490.74      494.390    490.1700        493.36  ...   \n",
       "3712           6.355       497.83      498.500    488.6800        490.85  ...   \n",
       "3713           3.870       491.18      491.690    487.5700        490.91  ...   \n",
       "3714           3.960       496.44      496.490    491.1901        491.25  ...   \n",
       "3715           2.810       494.06      495.570    493.3000        494.47  ...   \n",
       "\n",
       "      gold low  gold close  gold volume  year  month  day  day_of_week  \\\n",
       "0       110.79      112.03   18305238.0  2010      1   14            3   \n",
       "1       110.38      110.86   18000724.0  2010      1   15            4   \n",
       "2       110.83      111.52   10467927.0  2010      1   19            1   \n",
       "3       108.46      108.94   17534231.0  2010      1   20            2   \n",
       "4       106.61      107.37   25747831.0  2010      1   21            3   \n",
       "...        ...         ...          ...   ...    ...  ...          ...   \n",
       "3710    244.47      245.47    5789546.0  2024     10   11            4   \n",
       "3712    244.53      245.92    5640831.0  2024     10   15            1   \n",
       "3713    246.36      247.15    5431939.0  2024     10   16            2   \n",
       "3714    247.62      248.63    5176170.0  2024     10   17            3   \n",
       "3715    249.90      251.27    7833614.0  2024     10   18            4   \n",
       "\n",
       "      day_of_year  month_sin  month_cos  \n",
       "0              14   0.500000   0.866025  \n",
       "1              15   0.500000   0.866025  \n",
       "2              19   0.500000   0.866025  \n",
       "3              20   0.500000   0.866025  \n",
       "4              21   0.500000   0.866025  \n",
       "...           ...        ...        ...  \n",
       "3710          285  -0.866025   0.500000  \n",
       "3712          289  -0.866025   0.500000  \n",
       "3713          290  -0.866025   0.500000  \n",
       "3714          291  -0.866025   0.500000  \n",
       "3715          292  -0.866025   0.500000  \n",
       "\n",
       "[3677 rows x 50 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {\n",
    "  \"Linear Regression\": LinearRegression(),\n",
    "  # \"Ridge\": Ridge(alpha=0.1),\n",
    "  # \"Lasso\": Lasso(alpha=0.1),\n",
    "  \"Polynomial Regression (Degree 2)\": make_pipeline(PolynomialFeatures(degree=2), LinearRegression()),\n",
    "  \"Polynomial Regression (Degree 3)\": make_pipeline(PolynomialFeatures(degree=3), LinearRegression()),\n",
    "  # \"GAM (Linear Splines)\": LinearGAM(s(0, spline_order=1)),  # Linear splines\n",
    "  # \"GAM (Cubic Splines)\": LinearGAM(s(0, spline_order=3)),  # Cubic splines\n",
    "  # \"GAM (Tensor Splines)\": LinearGAM(te(0, 1)),  # Tensor splines for first two features\n",
    "  # \"Smoothing Splines\": LinearGAM(s(0)).gridsearch,  # Automatic smoothing\n",
    "}\n",
    "\n",
    "data = pd.read_csv('financial_regression_cleaned.csv')\n",
    "\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "\n",
    "\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day\n",
    "data['day_of_week'] = data['date'].dt.dayofweek\n",
    "data['day_of_year'] = data['date'].dt.dayofyear\n",
    "\n",
    "# Encode cyclical features \n",
    "data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\n",
    "data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)\n",
    "\n",
    "# Drop the original datetime column\n",
    "data = data.drop(columns=['date'])\n",
    "data = data.drop(data.columns[0], axis=1)  # Drop the first column (index 0)\n",
    "\n",
    "\n",
    "data.dropna(subset=['usd_chf', 'eur_usd'], inplace=True)\n",
    "\n",
    "print(data[['usd_chf', 'eur_usd']].isnull().sum())\n",
    "\n",
    "\n",
    "\n",
    "print(list(data.columns))\n",
    "\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_selector(data, prediction):\n",
    "\n",
    "  X = data.drop(columns=[prediction])  # Predictors\n",
    "  y = data[prediction]  # Target variable\n",
    "\n",
    "  # Split the data\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "  \n",
    "\n",
    "\n",
    "  best_models = {}\n",
    "  for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "      \n",
    "    remaining_features = list(X.columns)\n",
    "    selected_features = []\n",
    "    best_score = float('inf')\n",
    "\n",
    "    predictors = {} \n",
    "    while remaining_features:\n",
    "      scores = {}\n",
    "      for feature in remaining_features:\n",
    "        features_to_test = selected_features + [feature]\n",
    "          \n",
    "        # Special case for GAMs and Smoothing Splines\n",
    "        if \"GAM\" in model_name or \"Smoothing\" in model_name:\n",
    "          if \"Tensor\" in model_name and len(features_to_test) >= 2:\n",
    "            gam_model = LinearGAM(te(features_to_test[0], features_to_test[1])).gridsearch(\n",
    "              X_train[features_to_test].values, y_train.values\n",
    "            )\n",
    "          else:\n",
    "            gam_model = LinearGAM(s(0)).gridsearch(\n",
    "              X_train[features_to_test].values, y_train.values\n",
    "            )\n",
    "          score = -cross_val_score(\n",
    "            gam_model, \n",
    "            X_train[features_to_test].values, \n",
    "            y_train.values, \n",
    "            scoring='neg_mean_squared_error', \n",
    "            cv=5\n",
    "          ).mean()\n",
    "        else:\n",
    "          score = -cross_val_score(\n",
    "            model, \n",
    "            X_train[features_to_test], \n",
    "            y_train, \n",
    "            scoring='neg_mean_squared_error', \n",
    "            cv=5\n",
    "          ).mean()\n",
    "        \n",
    "        scores[feature] = score\n",
    "    \n",
    "      # Select the best feature for this iteration\n",
    "      new_feature = min(scores, key=scores.get)\n",
    "      new_score = scores[new_feature]\n",
    "    \n",
    "      best_score = new_score\n",
    "      selected_features.append(new_feature)\n",
    "      remaining_features.remove(new_feature)\n",
    "\n",
    "      predictors[tuple(selected_features)] = best_score\n",
    "\n",
    "        \n",
    "      #print(f\"  Selected feature: {new_feature}, CV Score: {best_score}\")\n",
    "\n",
    "    \n",
    "\n",
    "    selected_features = list(min(predictors, key = predictors.get))\n",
    "      \n",
    "    # Train the final model with selected features\n",
    "    if \"GAM\" in model_name or \"Smoothing\" in model_name:\n",
    "      if \"Tensor\" in model_name and len(selected_features) >= 2:\n",
    "        final_model = LinearGAM(te(selected_features[0], selected_features[1])).gridsearch(\n",
    "          X_train[selected_features].values, y_train.values\n",
    "        )\n",
    "      else:\n",
    "        final_model = LinearGAM(s(0)).gridsearch(\n",
    "          X_train[selected_features].values, y_train.values\n",
    "        )\n",
    "    else:\n",
    "      final_model = model\n",
    "      final_model.fit(X_train[selected_features], y_train)\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = final_model.predict(X_test[selected_features])\n",
    "    test_score = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"  Test MSE for {model_name}: {test_score} with selected features: {selected_features}\")\n",
    "    \n",
    "    \n",
    "    best_models[model_name] = {\n",
    "      \"model\": final_model,\n",
    "      \"selected_features\": selected_features,\n",
    "      \"test_score\": test_score\n",
    "    }\n",
    "\n",
    "  # Print summary of best-performing models\n",
    "  print(\"\\nSummary of Best Models:\")\n",
    "  for model_name, details in best_models.items():\n",
    "    print(f\"{model_name} - Test MSE: {details['test_score']}, Features: {details['selected_features']}\")\n",
    "\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model for predicting sp500 open is \n",
      "\n",
      "Evaluating model: Linear Regression\n",
      "  Test MSE for Linear Regression: 0.38753569238131563 with selected features: ['sp500 high', 'sp500 high-low', 'sp500 close', 'nasdaq volume', 'palladium high-low', 'oil close', 'oil open', 'oil high', 'oil low', 'platinum high-low', 'sp500 volume', 'nasdaq open', 'nasdaq high', 'nasdaq low', 'month_cos', 'day', 'oil high-low', 'nasdaq high-low', 'sp500 low', 'palladium volume', 'usd_chf', 'day_of_week', 'month', 'month_sin', 'silver high-low', 'platinum volume', 'gold high', 'platinum close', 'platinum open', 'platinum high']\n",
      "\n",
      "Summary of Best Models:\n",
      "Linear Regression - Test MSE: 0.38753569238131563, Features: ['sp500 high', 'sp500 high-low', 'sp500 close', 'nasdaq volume', 'palladium high-low', 'oil close', 'oil open', 'oil high', 'oil low', 'platinum high-low', 'sp500 volume', 'nasdaq open', 'nasdaq high', 'nasdaq low', 'month_cos', 'day', 'oil high-low', 'nasdaq high-low', 'sp500 low', 'palladium volume', 'usd_chf', 'day_of_week', 'month', 'month_sin', 'silver high-low', 'platinum volume', 'gold high', 'platinum close', 'platinum open', 'platinum high']\n",
      "\n",
      "\n",
      "Best Model for predicting sp500 high is \n",
      "\n",
      "Evaluating model: Linear Regression\n",
      "  Test MSE for Linear Regression: 1.580657744941455e-26 with selected features: ['sp500 open', 'sp500 high-low', 'sp500 low', 'nasdaq open', 'month_cos', 'month_sin', 'gold high', 'palladium low', 'silver high-low', 'day', 'silver high', 'nasdaq low', 'day_of_year']\n",
      "\n",
      "Summary of Best Models:\n",
      "Linear Regression - Test MSE: 1.580657744941455e-26, Features: ['sp500 open', 'sp500 high-low', 'sp500 low', 'nasdaq open', 'month_cos', 'month_sin', 'gold high', 'palladium low', 'silver high-low', 'day', 'silver high', 'nasdaq low', 'day_of_year']\n",
      "\n",
      "\n",
      "Best Model for predicting sp500 low is \n",
      "\n",
      "Evaluating model: Linear Regression\n",
      "  Test MSE for Linear Regression: 1.2361930561016433e-26 with selected features: ['sp500 close', 'sp500 open', 'sp500 high-low', 'sp500 high', 'platinum low']\n",
      "\n",
      "Summary of Best Models:\n",
      "Linear Regression - Test MSE: 1.2361930561016433e-26, Features: ['sp500 close', 'sp500 open', 'sp500 high-low', 'sp500 high', 'platinum low']\n",
      "\n",
      "\n",
      "Best Model for predicting sp500 close is \n",
      "\n",
      "Evaluating model: Linear Regression\n",
      "  Test MSE for Linear Regression: 0.314800448294393 with selected features: ['sp500 low', 'sp500 high-low', 'sp500 open', 'nasdaq close', 'nasdaq low', 'nasdaq high-low', 'nasdaq open', 'day_of_week', 'oil close', 'oil high', 'usd_chf', 'day', 'oil high-low', 'month', 'month_cos', 'palladium low', 'sp500 high', 'oil low', 'nasdaq high', 'palladium close', 'silver close', 'silver low']\n",
      "\n",
      "Summary of Best Models:\n",
      "Linear Regression - Test MSE: 0.314800448294393, Features: ['sp500 low', 'sp500 high-low', 'sp500 open', 'nasdaq close', 'nasdaq low', 'nasdaq high-low', 'nasdaq open', 'day_of_week', 'oil close', 'oil high', 'usd_chf', 'day', 'oil high-low', 'month', 'month_cos', 'palladium low', 'sp500 high', 'oil low', 'nasdaq high', 'palladium close', 'silver close', 'silver low']\n",
      "\n",
      "\n",
      "Best Model for predicting sp500 volume is \n",
      "\n",
      "Evaluating model: Linear Regression\n",
      "  Test MSE for Linear Regression: 853572467701504.8 with selected features: ['nasdaq volume', 'sp500 low', 'gold volume', 'usd_chf', 'eur_usd', 'year', 'sp500 high', 'nasdaq high-low', 'oil close', 'nasdaq low', 'palladium low', 'silver volume', 'day_of_week', 'palladium volume', 'sp500 close', 'nasdaq close', 'gold high', 'gold low', 'sp500 open', 'platinum volume', 'month_sin', 'oil low', 'palladium open']\n",
      "\n",
      "Summary of Best Models:\n",
      "Linear Regression - Test MSE: 853572467701504.8, Features: ['nasdaq volume', 'sp500 low', 'gold volume', 'usd_chf', 'eur_usd', 'year', 'sp500 high', 'nasdaq high-low', 'oil close', 'nasdaq low', 'palladium low', 'silver volume', 'day_of_week', 'palladium volume', 'sp500 close', 'nasdaq close', 'gold high', 'gold low', 'sp500 open', 'platinum volume', 'month_sin', 'oil low', 'palladium open']\n",
      "\n",
      "\n",
      "Best Model for predicting sp500 high-low is \n",
      "\n",
      "Evaluating model: Linear Regression\n",
      "  Test MSE for Linear Regression: 1.1104340452540277e-25 with selected features: ['nasdaq high-low', 'sp500 volume', 'silver low', 'nasdaq volume', 'palladium high-low', 'nasdaq open', 'year', 'gold high', 'month', 'nasdaq high', 'gold volume', 'gold low', 'palladium low', 'month_cos', 'sp500 low', 'sp500 high', 'silver volume', 'platinum open', 'platinum low', 'platinum high', 'silver open', 'silver close']\n",
      "\n",
      "Summary of Best Models:\n",
      "Linear Regression - Test MSE: 1.1104340452540277e-25, Features: ['nasdaq high-low', 'sp500 volume', 'silver low', 'nasdaq volume', 'palladium high-low', 'nasdaq open', 'year', 'gold high', 'month', 'nasdaq high', 'gold volume', 'gold low', 'palladium low', 'month_cos', 'sp500 low', 'sp500 high', 'silver volume', 'platinum open', 'platinum low', 'platinum high', 'silver open', 'silver close']\n",
      "\n",
      "\n",
      "Best Model for predicting nasdaq open is \n",
      "\n",
      "Evaluating model: Linear Regression\n",
      "  Test MSE for Linear Regression: 0.3428951767084802 with selected features: ['nasdaq high', 'nasdaq high-low', 'nasdaq close', 'sp500 high-low', 'oil close', 'year', 'sp500 open', 'sp500 high', 'sp500 close', 'usd_chf', 'day_of_year', 'oil open', 'month_cos', 'palladium volume', 'nasdaq low', 'sp500 low']\n",
      "\n",
      "Summary of Best Models:\n",
      "Linear Regression - Test MSE: 0.3428951767084802, Features: ['nasdaq high', 'nasdaq high-low', 'nasdaq close', 'sp500 high-low', 'oil close', 'year', 'sp500 open', 'sp500 high', 'sp500 close', 'usd_chf', 'day_of_year', 'oil open', 'month_cos', 'palladium volume', 'nasdaq low', 'sp500 low']\n",
      "\n",
      "\n",
      "Best Model for predicting nasdaq high is \n",
      "\n",
      "Evaluating model: Linear Regression\n",
      "  Test MSE for Linear Regression: 1.5742919804193236e-27 with selected features: ['nasdaq open', 'nasdaq close', 'nasdaq high-low', 'nasdaq low', 'month', 'gold low', 'day_of_week', 'palladium high-low']\n",
      "\n",
      "Summary of Best Models:\n",
      "Linear Regression - Test MSE: 1.5742919804193236e-27, Features: ['nasdaq open', 'nasdaq close', 'nasdaq high-low', 'nasdaq low', 'month', 'gold low', 'day_of_week', 'palladium high-low']\n",
      "\n",
      "\n",
      "Best Model for predicting nasdaq low is \n",
      "\n",
      "Evaluating model: Linear Regression\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m targets:\n\u001b[0;32m      4\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Model for predicting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m   model_selector(data, target)\n\u001b[0;32m      8\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[89], line 43\u001b[0m, in \u001b[0;36mmodel_selector\u001b[1;34m(data, prediction)\u001b[0m\n\u001b[0;32m     35\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mcross_val_score(\n\u001b[0;32m     36\u001b[0m       gam_model, \n\u001b[0;32m     37\u001b[0m       X_train[features_to_test]\u001b[38;5;241m.\u001b[39mvalues, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m       cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     41\u001b[0m     )\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     42\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 43\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mcross_val_score(\n\u001b[0;32m     44\u001b[0m       model, \n\u001b[0;32m     45\u001b[0m       X_train[features_to_test], \n\u001b[0;32m     46\u001b[0m       y_train, \n\u001b[0;32m     47\u001b[0m       scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     48\u001b[0m       cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     49\u001b[0m     )\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     51\u001b[0m   scores[feature] \u001b[38;5;241m=\u001b[39m score\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Select the best feature for this iteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:562\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    560\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 562\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[0;32m    563\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    564\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    565\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    566\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    567\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[0;32m    568\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[0;32m    569\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    570\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    571\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[0;32m    572\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[0;32m    573\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    574\u001b[0m )\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:309\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 309\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    310\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    311\u001b[0m         clone(estimator),\n\u001b[0;32m    312\u001b[0m         X,\n\u001b[0;32m    313\u001b[0m         y,\n\u001b[0;32m    314\u001b[0m         scorers,\n\u001b[0;32m    315\u001b[0m         train,\n\u001b[0;32m    316\u001b[0m         test,\n\u001b[0;32m    317\u001b[0m         verbose,\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    319\u001b[0m         fit_params,\n\u001b[0;32m    320\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[0;32m    321\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    322\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[0;32m    323\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    324\u001b[0m     )\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[0;32m    326\u001b[0m )\n\u001b[0;32m    328\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:754\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    751\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_error\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    753\u001b[0m fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m--> 754\u001b[0m test_scores \u001b[38;5;241m=\u001b[39m _score(estimator, X_test, y_test, scorer, error_score)\n\u001b[0;32m    755\u001b[0m score_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;241m-\u001b[39m fit_time\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_train_score:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:813\u001b[0m, in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[0;32m    811\u001b[0m         scores \u001b[38;5;241m=\u001b[39m scorer(estimator, X_test)\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 813\u001b[0m         scores \u001b[38;5;241m=\u001b[39m scorer(estimator, X_test, y_test)\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scorer, _MultimetricScorer):\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;66;03m# If `_MultimetricScorer` raises exception, the `error_score`\u001b[39;00m\n\u001b[0;32m    817\u001b[0m         \u001b[38;5;66;03m# parameter is equal to \"raise\".\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:136\u001b[0m, in \u001b[0;36m_MultimetricScorer.__call__\u001b[1;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scorer, _BaseScorer):\n\u001b[1;32m--> 136\u001b[0m         score \u001b[38;5;241m=\u001b[39m scorer\u001b[38;5;241m.\u001b[39m_score(\n\u001b[0;32m    137\u001b[0m             cached_call, estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mget(name)\u001b[38;5;241m.\u001b[39mscore\n\u001b[0;32m    138\u001b[0m         )\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m         score \u001b[38;5;241m=\u001b[39m scorer(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mget(name)\u001b[38;5;241m.\u001b[39mscore)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:353\u001b[0m, in \u001b[0;36m_PredictScorer._score\u001b[1;34m(self, method_caller, estimator, X, y_true, **kwargs)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate predicted target values for X relative to y_true.\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \n\u001b[0;32m    318\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;124;03m    Score function applied to prediction of estimator on X.\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_overlap(\n\u001b[0;32m    346\u001b[0m     message\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is an overlap between set kwargs of this scorer instance and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    351\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m    352\u001b[0m )\n\u001b[1;32m--> 353\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m method_caller(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m, X)\n\u001b[0;32m    354\u001b[0m scoring_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sign \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_score_func(y_true, y_pred, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscoring_kwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:86\u001b[0m, in \u001b[0;36m_cached_call\u001b[1;34m(cache, estimator, response_method, *args, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m response_method \u001b[38;5;129;01min\u001b[39;00m cache:\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cache[response_method]\n\u001b[1;32m---> 86\u001b[0m result, _ \u001b[38;5;241m=\u001b[39m _get_response_values(\n\u001b[0;32m     87\u001b[0m     estimator, \u001b[38;5;241m*\u001b[39margs, response_method\u001b[38;5;241m=\u001b[39mresponse_method, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     88\u001b[0m )\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     cache[response_method] \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_response.py:109\u001b[0m, in \u001b[0;36m_get_response_values\u001b[1;34m(estimator, X, response_method, pos_label)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response_method \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    104\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should either be a classifier to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused with response_method=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or the response_method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got a regressor with response_method=\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m         )\n\u001b[1;32m--> 109\u001b[0m     y_pred, pos_label \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mpredict(X), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_pred, pos_label\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:386\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    373\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decision_function(X)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:369\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    367\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 369\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m], reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[0;32m    960\u001b[0m             array,\n\u001b[0;32m    961\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    962\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    963\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    964\u001b[0m         )\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# First try an O(n) time, O(1) space solution for the common case that\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# everything is finite; fall back to O(n) space `np.isinf/isnan` or custom\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Cython implementation to prevent false positives and provide a detailed\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# error message.\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 120\u001b[0m     first_pass_isfinite \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39misfinite(xp\u001b[38;5;241m.\u001b[39msum(X))\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np\u001b[38;5;241m.\u001b[39madd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m, axis, dtype, out, keepdims\u001b[38;5;241m=\u001b[39mkeepdims,\n\u001b[0;32m   2314\u001b[0m                       initial\u001b[38;5;241m=\u001b[39minitial, where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "targets = [\"sp500 open\",\"sp500 high\",\"sp500 low\",\"sp500 close\",\"sp500 volume\",\"sp500 high-low\",\"nasdaq open\",\"nasdaq high\",\"nasdaq low\",\"nasdaq close\",\"nasdaq volume\",\"nasdaq high-low\",\"silver open\",\"silver high\",\"silver low\",\"silver close\",\"silver volume\",\"silver high-low\",\"oil open\",\"oil high\",\"oil low\",\"oil close\",\"oil volume\",\"oil high-low\",\"platinum open\",\"platinum high\",\"platinum low\",\"platinum close\",\"platinum volume\",\"platinum high-low\",\"palladium open\",\"palladium high\",\"palladium low\",\"palladium close\",\"palladium volume\",\"palladium high-low\",\"gold open\",\"gold high\",\"gold low\",\"gold close\",\"gold volume\"]\n",
    "\n",
    "for target in targets:\n",
    "  print(f\"Best Model for predicting {target} is \")\n",
    "  \n",
    "  model_selector(data, target)\n",
    "\n",
    "  print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
